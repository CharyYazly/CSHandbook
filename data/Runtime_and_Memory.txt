## Introduction

Computers are super fast at making calculations compared to humans, but humans have much more "memory" than computers currently do. For example, computers can add two 100-digit numbers together much more quickly than any human possibly can. However, the human brain can contain much more memory than humans.

Brains have trillions of connections between neurons and the estimated storage capacity is about 2.5 petabytes. That is approximately 340 years of TV shows that you could watch! Computers on the other hand, have much less memory than human brains. Standard computers have around 8GB of RAM and some higher end machines may have 16-32GB. Although hard-disks can store terabytes of memory, we use RAM (flash memory) when analyzing computer memory because it is much faster. As an analogy, RAM can be thought of as grabbing an object in another room whereas disk memory is driving 20 min away to get that object. 

The neurons and connections in our brains allow us to store an immense amount of information in our brains and allows us to easily recognize patterns much better than computers. For example it is very easy for us to identify different objects around us (e.g. we can easily differentiate between an apple and an orange), but it is difficult for a computer to do this for the countless number of objects in the world. However this is currently changing as more research is being done in "deep learning".

|Brain|Computer
-|-|-
Memory|2.5 petabytes (approx. 340 years of TV shows)|16GB RAM
Processing| 60Hz | 2.7GHz Quad Core (1,000,000,000 times faster than humans)

## Limits

There are many different methods of implementing different things but most of the time we care most about implementations that are the fastest and use the least amount of memory. Let's go through some basic benchmarks about computers that you should know. Adding two numbers takes a nanosecond (1 billionth of a second) for an average computer to process. For practical purposes, we'll assume that the average computer program can hold up to 1GB or RAM or about 250 million integers. We use RAM because it is flash memory and read/write is super fast in comparison to disk read/writes. 

Memory| Operations (per seconds)
-|--
1 GB (250,000,000 ints) | 100,000,000 operations

## Big O Notation

When we compare how efficient algorithms or data structures are to another, we want to be able to describe them such that they can be quantified. We use something called Big O notation to do this. Many algorithms and data structures will have its time and space complexities depend on the size of the inputs. The Big O notation takes the largest factor of an input to compare computation times / memory usage. When we take the largest factor, we ignore smaller factors, coefficients and constants because they do not matter at very large values. For example: O(3n^^2^^ + 12n + 20) is simply O(n^^2^^) because it is the largest factor.

Here is a list of common Big O notations based on complexity:

Big O | Limit of N for 1 second (Standard processor)
-|-
O(1) *Constant Time* | runtime independent of N
O(log N) *Sublinear time* | a very big number
O(N) *Linear Time* | 100,000,000
O(N log N) | 5,000,000
O(N^^2^^) *Quadractic Time* | 10000
O(N^^3^^) *Cubic Time* | 450
O(2^^N^^) *Exponential Time* | 27
O(N!) *Factorial Time* | 11

Keep in mind that in a couple of years as technology improves, this chart will be outdated.

## Runtime Analysis

Let us examine a function that takes in an array of size N and returns the maximum element. In a program we are usually concerned with two complexities: memory and time. 

Code:

[[[[
int findMax(int[] array){
  int maxVal = array[0]; //O(1) memory to store max element, O(1) time for assignment
  for(int i=1;i<array.length;i++){ //O(n) loop runtime
      if(maxVal < array[i]){ //O(1) to compare runtime
         maxVal = array[i]; //O(1) to assign new value runtime
     }
  }
}
]]]]

Memory: The array takes O(N) memory and storing the max value is O(1) more memory. but usually when analyzing programs, we ignore the input memory sizes and take into account additional memory that is required to produce the output. So the memory footprint of the function is O(1).

Time: The first assignment of maxVal takes O(1). The loop runs N-1 times and of each of those N-1 times it checks if the current array element is greater than the current max which takes O(1). If it is greater, then we reassign maxVal which is O(1). When analyzing time complexity, we usually take worst case so we have: O(1+(N-1)(2)) and this simplifies to O(N) since it is the largest factor.

### Example

[[[[
void zeroArrays(int[][] grid){
  for(int i=0; i<grid.length;i++){
    for(int j=0;j<grid[i].length;j++){
      grid[i][j] = 0;
    }
  }
}
]]]]

